# ğŸŒŒ Neon Loom

**Weaving live intelligence into autonomous AI operations** â€” a complete platform for building and operating intelligent AI systems.

> ğŸš€ **Quick Start:** `bash <(curl -fsSL https://raw.githubusercontent.com/Neon-Loom/Neon-Loom/main/scripts/install.sh)`  
> ğŸ“– **[Full Installation Guide](https://github.com/Neon-Loom/.github/blob/main/INSTALLATION.md)** | Specialized for **Debian/Ubuntu** systems

## What is Neon Loom?

Neon Loom transforms your documentation and conversations into continuously-improving AI models. It's your personal AI training and deployment platform that ingests knowledge, distills datasets, trains custom models, and serves them through beautiful web interfacesâ€”all with automated nightly cycles.

**Core Platform:** [Neon-Loom/Neon-Loom](https://github.com/Neon-Loom/Neon-Loom) â€” Distillation, training, browser agent, and orchestration

## ğŸ—ï¸ Ecosystem Components

<table><tr><td>

### ğŸš€ Inference and Orchestration

- ğŸ”¹ [Neon-Loom/inference-stack-ops](https://github.com/Neon-Loom/inference-stack-ops)
- ğŸ”¹ [Neon-Loom/superagi-gpu-ops](https://github.com/Neon-Loom/superagi-gpu-ops)

</td><td>

### ğŸ’¬ Chat Interfaces

- ğŸ’¬ [Neon-Loom/neon-openwebui-ops](https://github.com/Neon-Loom/neon-openwebui-ops)
- ğŸ’¬ [Neon-Loom/librechat-ops](https://github.com/Neon-Loom/librechat-ops)

</td></tr></table>

### ğŸ§ª Labs & Experimental

- ğŸ§ª [Neon-Loom/urban-octo-giggle](https://github.com/Neon-Loom/urban-octo-giggle)
- ğŸ§¼ [Neon-Loom/sanitized_agi_ollama](https://github.com/Neon-Loom/sanitized_agi_ollama)

## ğŸ¯ Getting Started

### System Requirements

- **OS:** Ubuntu 22.04+ or Debian 12+ *(installation specialized for Debian-based systems)*
- **RAM:** 8GB minimum (16GB+ recommended)
- **Storage:** 50GB+ free space
- **GPU:** NVIDIA GPU recommended (optional)

### Installation

**Automated Installer (Recommended):**

```bash
bash <(curl -fsSL https://raw.githubusercontent.com/Neon-Loom/Neon-Loom/main/scripts/install.sh)
```

The installer will interactively ask about:
- Installation directory (default: `~/neon-loom`)
- SSL/TLS setup (Caddy vs Certbot)
- Firewall configuration (UFW vs IPTables)
- Component selection and network options

**Manual Installation:**

For advanced users who prefer complete control, see the [Installation Guide](https://github.com/Neon-Loom/.github/blob/main/INSTALLATION.md).

### Default Installation Structure

Neon Loom installs to `~/neon-loom/` by default, containing:
```
~/neon-loom/
â”œâ”€â”€ config.env          # Main configuration
â”œâ”€â”€ sources/            # Documentation to ingest
â”œâ”€â”€ data/               # Training datasets
â”œâ”€â”€ output/             # Model outputs
â”œâ”€â”€ logs/               # System logs
â””â”€â”€ neon-loom/          # Core platform repository
```

This location is customizable during installation.

## ğŸ“š Documentation

- ğŸ“– **[Installation Guide](https://github.com/Neon-Loom/.github/blob/main/INSTALLATION.md)** â€” Complete installation reference
- ğŸ“– **[Setup Guide](https://github.com/Neon-Loom/Neon-Loom/blob/main/docs/SETUP_GUIDE.md)** â€” Manual configuration
- ğŸ“– **[Networking Setup](https://github.com/Neon-Loom/Neon-Loom/blob/main/docs/NETWORKING_SETUP.md)** â€” Firewalls, reverse proxies, SSL
- ğŸ“– **[Platform Layout](https://github.com/Neon-Loom/Neon-Loom/blob/main/docs/PLATFORM_LAYOUT.md)** â€” Architecture overview

## ğŸ’¡ Key Features

- ğŸ”„ **Fully Automated** â€” Nightly training cycles
- ğŸš€ **GPU Accelerated** â€” NVIDIA GPU support
- ğŸŒ **Web Browsing AI** â€” Live data from the web
- ğŸ’¾ **Continuous Learning** â€” Learns from every conversation
- ğŸ“Š **Monitoring Built-in** â€” Health dashboards
- ğŸ”’ **Security First** â€” No secrets in code
- ğŸ³ **Docker Ready** â€” Containerized services

## ğŸ¤ Contributing

We welcome contributions! Each repository has its own contribution guidelines. Generally:

- Keep secrets out of Git
- Update documentation with code changes
- Run tests before submitting PRs
- Follow existing code style

## ğŸ“„ License

Neon Loom projects are released under the MIT License. See individual repositories for details.

---

> **Note:** Each repository contains specific deployment notes, GPU requirements, and operational runbooks. Check individual READMEs for component-specific details.
